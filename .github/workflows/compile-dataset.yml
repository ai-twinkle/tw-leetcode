name: Compile Dataset

on:
  push:
    branches: [ main ]
    paths:
      - '*/questionCode.ts'
      - '*/answer.ts'
      - '*/Note.md'
  schedule:
    # Run every Monday at 12:00 UTC
    - cron: '0 12 * * 1'
  workflow_dispatch:

permissions:
  contents: write

jobs:
  compile:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'

      - name: Install dependencies
        run: |
          cd scripts
          npm install

      - name: Compile dataset
        run: node scripts/compile-dataset.js

      - name: Show dataset stats
        run: |
          echo "Dataset compilation completed!"
          echo "Dataset size: $(wc -l < data/datasets.jsonl) problems"
          echo "File size: $(du -h data/datasets.jsonl | cut -f1)"
          echo ""
          echo "Sample entries:"
          head -2 data/datasets.jsonl | jq '.'

      - name: Validate JSONL format
        run: |
          echo "Validating JSONL format..."
          node -e "
            const fs = require('fs');
            const readline = require('readline');
          
            async function validateJSONL() {
              const fileStream = fs.createReadStream('data/datasets.jsonl');
              const rl = readline.createInterface({
                input: fileStream,
                crlfDelay: Infinity
              });

              let lineNumber = 0;
              let validEntries = 0;
              let errors = 0;
          
              for await (const line of rl) {
                lineNumber++;
                try {
                  const data = JSON.parse(line);
                  if (!data.text || !data.question || !data.constraints || !data.thought || !data.answer || !data.src || !data.time_complexity || !data.space_complexity) {
                    console.error(\`Line \${lineNumber}: Missing required fields\`);
                    errors++;
                  } else {
                    validEntries++;
                  }
                } catch (e) {
                  console.error(\`Line \${lineNumber}: Invalid JSON - \${e.message}\`);
                  errors++;
                }
              }
          
              console.log(\`Validation complete: \${validEntries} valid entries, \${errors} errors\`);
              if (errors > 0) {
                process.exit(1);
              }
            }
          
            validateJSONL();
          "

      - name: Create dataset branch
        run: |
          # Create or switch to dataset branch
          git checkout -B dataset
          
          # Backup the compiled dataset and README before removing files
          cp data/datasets.jsonl datasets-backup.jsonl
          cp scripts/README.md.dataset readme-backup.md

          # Remove all files except the ones we want to keep
          git rm -rf . || true

          # Restore essential files from main
          git checkout main -- .gitattributes .gitignore || true
          
          # Create data directory and restore the dataset and README
          mkdir -p data
          mv datasets-backup.jsonl data/datasets.jsonl
          mv readme-backup.md README.md
          
          # Update .gitignore for dataset branch
          echo "# Dataset branch - only contains compiled data" > .gitignore
          echo "node_modules/" >> .gitignore
          echo "*.log" >> .gitignore
          echo ".DS_Store" >> .gitignore
          
          # Create .gitattributes for large files
          echo "*.jsonl filter=lfs diff=lfs merge=lfs -text" > .gitattributes
          echo "data/*.jsonl filter=lfs diff=lfs merge=lfs -text" >> .gitattributes
          
          # Add and commit files
          git add .
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git commit -m "Update dataset - $(date -u +%Y-%m-%d)" || echo "No changes to commit"
          
          # Push to dataset branch with proper authentication
          git push https://${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git dataset --force

      # =========================
      # Hugging Face (only on schedule / dispatch)
      # =========================

      - name: Setup Python for Hugging Face
        if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install Hugging Face Hub (pin)
        if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
        run: |
          pip install --upgrade pip
          # Pin to a stable version line to avoid sudden CLI/API behavior changes
          pip install "huggingface_hub==0.24.6"

      - name: Switch to dataset branch for upload
        if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
        run: git checkout dataset

      - name: Ensure dataset repo exists
        if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          python - <<'PY'
          import os
          from huggingface_hub import create_repo
          create_repo(
              "twinkle-ai/tw-leetcode",
              repo_type="dataset",
              exist_ok=True,
              token=os.environ["HF_TOKEN"],
          )
          print("Repo ensured.")
          PY

      - name: Upload to Hugging Face (single PR, Python API)
        if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          set -e

          python - <<'PY'
          import os, datetime
          from huggingface_hub import HfApi

          token = os.environ["HF_TOKEN"]
          repo_id = "twinkle-ai/tw-leetcode"
          repo_type = "dataset"

          api = HfApi(token=token)

          # Base branch on Hub you want the PR against:
          base_branch = "main"
          commit_msg = f"Auto-update dataset - {datetime.datetime.utcnow().strftime('%Y-%m-%d')}"

          # We are on the local 'dataset' branch (only contains data/, README.md, .gitattributes, .gitignore)
          api.upload_folder(
              repo_id=repo_id,
              repo_type=repo_type,
              folder_path=".",
              path_in_repo=".",
              revision=base_branch,
              create_pr=True,
              commit_message=commit_msg,
          )
          print("Upload complete (PR created).")
          PY
